{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"include_colab_link":true},"accelerator":"GPU","gpuClass":"standard","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":201268,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":171698,"modelId":194018}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"b202ec9d-7d3c-44c4-baf6-adfddc6b4237","cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/Page0526/Pytorch-crash-course/blob/main/deep-neural-networks/AutoEncoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github","colab_type":"text"}},{"id":"8e960184-af68-4d6b-81d9-41bfa6168b48","cell_type":"markdown","source":"# A Simple VAE and Latent Space Visualization with PyTorch","metadata":{"id":"bc7a67df"}},{"id":"b9551c52-6c5e-4b63-a375-8254297bb209","cell_type":"markdown","source":"Ý tưởng của VAE là encode input dưới latent space xong đó decode ra output (reconstructed image) gần giống với input. Ở đấy input sẽ được encoder as a distribution).\nFollow: https://medium.com/@outerrencedl/variational-autoencoder-and-a-bit-kl-divergence-with-pytorch-ce04fd55d0d7","metadata":{"id":"Ld5DLAPGbSGs"}},{"id":"2da611ac-a8b8-407d-8c79-9b6023bdb055","cell_type":"markdown","source":"## Preliminaries","metadata":{"id":"bZ5oWa89GOy7"}},{"id":"0d25ba46-f5ee-4352-a965-ce8217c2288e","cell_type":"code","source":"!pip install -q lightning wandb torchvision torchinfo torchsummary","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d8eaae9a-f1ea-4f5c-922d-a5a1ec07d4ec","cell_type":"code","source":"import torch\nfrom torch import nn\nimport torch.nn.functional as F\n\n# torchsummary for easy checking and debugging\nfrom torchsummary import summary\n# torchvision for downloading and processing data\nimport torchvision\nfrom torchvision import transforms\n\n# Other for notebook UI and latent space visualization\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport wandb \nimport pytorch_lightning as pl\nfrom pytz import timezone\nfrom datetime import datetime\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\nfrom PIL import Image\nfrom torchmetrics.image import MaxMetric, MeanMetric, PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure\nfrom typing import Any, Dict, Tuple\nfrom torchmetrics.classification.accuracy import Accuracy","metadata":{"id":"1525c069","execution":{"iopub.status.busy":"2024-12-17T11:55:36.180832Z","iopub.execute_input":"2024-12-17T11:55:36.181613Z","iopub.status.idle":"2024-12-17T11:55:40.234723Z","shell.execute_reply.started":"2024-12-17T11:55:36.181573Z","shell.execute_reply":"2024-12-17T11:55:40.233890Z"},"trusted":true},"outputs":[],"execution_count":1},{"id":"95a8a34a-4048-4050-9907-ac5f4a539825","cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nwandb_api_key = user_secrets.get_secret(\"wandb_api_key\")\nwandb.login(key=wandb_api_key)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T11:55:55.813415Z","iopub.execute_input":"2024-12-17T11:55:55.813973Z","iopub.status.idle":"2024-12-17T11:55:57.899188Z","shell.execute_reply.started":"2024-12-17T11:55:55.813938Z","shell.execute_reply":"2024-12-17T11:55:57.898337Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpage0526\u001b[0m (\u001b[33miai-uet-vnu\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"id":"0af65ff7-c0e1-4f15-9bc5-8162e12ec01a","cell_type":"markdown","source":"## Gaussian distribution\n$f(x)=\\frac1 {\\sigma \\sqrt{2\\pi}}e^{-\\frac1 2 (\\frac{x-\\mu}{\\mu})^2}$","metadata":{}},{"id":"0ed52520-e736-441d-8850-81575e802dcf","cell_type":"code","source":"# sampling normal distribution\ndef normal_sample(x, mean, var):\n    return (1/(np.sqrt(2* np.pi * var))) * (np.exp(-0.5 * (x-mean)**2 / var))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7c68c623-c8d3-40c5-8826-170c9d2380c8","cell_type":"code","source":"mean = [0.0,0.0,0.0,-2.0]\nvar = [0.2,1.0,5.0,0.5]\nx_range = np.arange(-5.0, 5.0, 0.05)\ncurves = []\n\n# visualize normal distribution\nfor i in range(len(mean)):\n    crv = [normal_sample(x, mean[i], var[i]) for x in x_range]\n    curves.append(crv)\n    \nplt.figure(figsize=(8,4))\nfor i in range(len(curves)):\n    plt.plot(x_range, curves[i], label=f\"$\\mu$={mean[i]}, $\\sigma^2$={var[i]}\")\n    \nplt.grid(\"on\")\nplt.legend()\nplt.show","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7cc52dd8-8edb-4d5b-ba3d-300e07cab483","cell_type":"markdown","source":"## KL Divergence\n$KL=(\\mathbb{N}(x|\\mu_1,\\sigma_1)||\\mathbb{N}(x|\\mu_2,\\sigma_2))=log\\frac{\\sigma_2}{\\sigma_1}+\\frac{\\sigma_1^2+(\\mu_1-\\mu_2)^2}{2\\sigma_2^2}-\\frac12$","metadata":{}},{"id":"06a5dbb8-0892-46dd-acdd-cd3ab4ef18d4","cell_type":"code","source":"kl_loss = lambda mean1, mean2, var1, var2:(\n        np.log((var2 / var1)**0.5) + (var1 + (mean1-mean2)**2) / (2 * var2) - 0.5)\nprint(\"The KL divergence between curve 0 and curve 1:\")\nprint(f\"{kl_loss(mean[0], mean[1], var[0], var[1]):.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"2a6cfb24-436b-4b34-a190-260e129da958","cell_type":"markdown","source":"## Point-wise KL divergence\n$KL(p||q)=\\sum^K_{k=1}p_klogp_k-\\sum^K_{k=1}p_klogq_k=\\sum^K_{k=1}p_klog\\frac{p_k}{q_k}$","metadata":{}},{"id":"c053672d-46d5-456b-9791-0f6ecd729976","cell_type":"code","source":"def pt_wise_kl(d_true, d_pred):\n    d_pred, d_true = np.array(d_pred), np.array(d_true)\n    return np.sum(d_true * np.log(d_true / d_pred))\n\nprint(f\"{pt_wise_kl(curves[0], curves[1]):.4f}\")\nprint(f\"{pt_wise_kl(curves[1], curves[0]):.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e1d0b8c4-08c9-49c2-afeb-87598afad62f","cell_type":"markdown","source":"## Loading FashionMNIST Dataset","metadata":{"id":"c87956ce"}},{"id":"fbdb7f84-23b8-47a1-b84a-c7f5704e9dae","cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torchvision import datasets\n\ntrainset = datasets.FashionMNIST(root=\"data\",\n                                 train=True,\n                                 download=True,\n                                 transform=transforms.Compose([\n                                     transforms.ToTensor(),\n                                 ]),\n                                 target_transform=None \n        )\n\ntestset = datasets.FashionMNIST(root=\"data\",\n                                train=False,\n                                download=True,\n                                transform=transforms.Compose([\n                                    transforms.ToTensor(),\n                                ]),\n                                target_transform=None # you can transform labels as well\n)","metadata":{"id":"pQNST75Kmnzz","outputId":"f83fcc91-8f5f-4b2b-d988-deefd1a79901","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2024-12-17T11:56:02.210486Z","iopub.execute_input":"2024-12-17T11:56:02.211071Z","iopub.status.idle":"2024-12-17T11:56:02.308166Z","shell.execute_reply.started":"2024-12-17T11:56:02.211032Z","shell.execute_reply":"2024-12-17T11:56:02.307193Z"},"trusted":true},"outputs":[],"execution_count":3},{"id":"b5a8025b-2541-4dbc-b48a-2916c7c33cc2","cell_type":"code","source":"# Checking if CUDA is available on current machine\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Deivce: {DEVICE}')\n\nBATCH_SIZE = 64\nEPOCHS = 30\nlr = 2e-3\nHIDDEN_DIM = 2\nlabels = trainset.classes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T11:56:03.258521Z","iopub.execute_input":"2024-12-17T11:56:03.259234Z","iopub.status.idle":"2024-12-17T11:56:03.295885Z","shell.execute_reply.started":"2024-12-17T11:56:03.259196Z","shell.execute_reply":"2024-12-17T11:56:03.295046Z"}},"outputs":[{"name":"stdout","text":"Deivce: cuda\n","output_type":"stream"}],"execution_count":4},{"id":"d37ecb4a-8c84-4df8-8f51-6173bd0b4fdf","cell_type":"code","source":"train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=3)\ntest_loader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T11:56:04.492448Z","iopub.execute_input":"2024-12-17T11:56:04.492845Z","iopub.status.idle":"2024-12-17T11:56:04.497575Z","shell.execute_reply.started":"2024-12-17T11:56:04.492810Z","shell.execute_reply":"2024-12-17T11:56:04.496660Z"}},"outputs":[],"execution_count":5},{"id":"7a872142-e814-4877-92aa-b7128906940c","cell_type":"code","source":"# Visualize data\nfor _, data in enumerate(train_loader):\n  print(\"Batch shape: \",data[0].shape)\n  fig, ax = plt.subplots(1, 4, figsize=(10, 4))\n\n  for i in range(4):\n    # Turn 3D tensor to 2D tensor due to image's single channel\n    ax[i].imshow(data[0][i].squeeze(), cmap='gray')\n    ax[i].axis(\"off\")\n    ax[i].set_title(labels[data[1][i]])\n\n  plt.show()\n  break","metadata":{"id":"kPdozU39mpo0","outputId":"4804e7f4-333c-4b9b-d70a-64117f9ef9d7","colab":{"base_uri":"https://localhost:8080/","height":245},"execution":{"iopub.status.busy":"2024-12-17T11:56:05.760543Z","iopub.execute_input":"2024-12-17T11:56:05.761376Z","iopub.status.idle":"2024-12-17T11:56:06.222491Z","shell.execute_reply.started":"2024-12-17T11:56:05.761336Z","shell.execute_reply":"2024-12-17T11:56:06.221615Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Batch shape:  torch.Size([64, 1, 28, 28])\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x400 with 4 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAxsAAADSCAYAAAAi0d0oAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAug0lEQVR4nO3deXQVdZrG8Tdk3wMhCSEsIWEHlZamiSNrE4kgbYPsLmwKDg10O6PjsbvHEZxWR9odm23GVhrGI6gggiyNI+oIMoIgCDaIEBZZkkASkhBICKn5g8NtLnnfQEEKAnw/53iOPNStqntTv6r65d77EOA4jiMAAAAAUMPqXO0dAAAAAHB9YrIBAAAAwBNMNgAAAAB4gskGAAAAAE8w2QAAAADgCSYbAAAAADzBZAMAAACAJ5hsAAAAAPAEkw0AAAAAnmCyUUvt2bNHAgIC5IUXXrjauwIAuMICAgJk8uTJvj+/9dZbEhAQIHv27Llq+wRcDW7uhyZPniwBAQFXYK/gxg092fj2229l0KBB0rRpUwkLC5OUlBS54447ZNq0aVd714BaJSAg4KL++/TTT6/2rgJXxdnJwNn/wsLCpGXLljJx4kTJycm52rsHeOZavT6UlpbK5MmTq92vgoICCQoKkgULFoiIyLPPPisffPDBldnB60jQ1d6Bq2Xt2rXSs2dPadKkiYwdO1YaNGgg+/fvl3Xr1smrr74qkyZNutq7CNQac+fO9fvzX/7yF1m1alWVvE2bNldyt4Ba5+mnn5ZmzZrJyZMn5YsvvpAZM2bIsmXLZOvWrRIREXG1dw+ocbXp+vCv//qv8sQTT1zUsqWlpTJlyhQREenRo4e6zMqVKyUgIEB69+4tImcmG4MGDZL+/fvXxO7eMG7YycYzzzwjsbGxsn79eomLi/P7u9zc3KuzU1dYaWkpFz9clPvvv9/vz+vWrZNVq1ZVyc93rR5jx48fl8jIyKu9G7gG9enTR37605+KiMhDDz0k8fHx8tJLL8nixYtl+PDhV3nvvMOYuXFd6vXBC0FBQRIUVP2tbWVlpZSXl1/U+pYtWya33357lftEuHPDfoxq165d0q5dO/UASkxM9P1/QECATJw4UT744ANp3769hIaGSrt27WTFihVVHnfgwAEZM2aMJCUl+Zb785//7LdMeXm5/Nu//Zt07NhRYmNjJTIyUrp27SqrV6++4D47jiPjxo2TkJAQWbhwoS+fN2+edOzYUcLDw6VevXoybNgw2b9/v99je/ToIe3bt5evv/5aunXrJhEREfK73/3ugtsELlZ1x1hubq48+OCDkpSUJGFhYXLLLbfInDlz/B7/6aefqm+1n/287ltvveXLDh8+LKNHj5ZGjRpJaGioJCcnyy9/+csqn2dfvny5dO3aVSIjIyU6Olruuusu2bZtm98yo0aNkqioKNm1a5f07dtXoqOj5b777qux1wU3tp///OciIpKdnS09evRQf4M6atQoSU1NvaT1T58+Xdq1ayehoaHSsGFDmTBhghQWFvr+fuLEiRIVFSWlpaVVHjt8+HBp0KCBnD592pcxZnClbdiwQbKysqR+/foSHh4uzZo1kzFjxqjLzp49W9LT0yU0NFQ6deok69ev9/t77TsbZ+/j/vu//9s3VmbOnCkJCQkiIjJlyhTfR73O/Z5UZWWlrFixQu666y7feo4fPy5z5szxLT9q1Cjf8ps2bZI+ffpITEyMREVFSa9evWTdunV++3L245aff/65PPzwwxIfHy8xMTEyYsQIKSgouNSXsNa7Yd/ZaNq0qXz55ZeydetWad++fbXLfvHFF7Jw4UL51a9+JdHR0fLaa6/JwIEDZd++fRIfHy8iIjk5OZKRkeE7qBMSEmT58uXy4IMPSlFRkTzyyCMiIlJUVCT/9V//JcOHD5exY8dKcXGxvPHGG5KVlSVfffWVdOjQQd2H06dPy5gxY2T+/PmyaNEi38H/zDPPyJNPPilDhgyRhx56SPLy8mTatGnSrVs32bRpk99k6ujRo9KnTx8ZNmyY3H///ZKUlHTZryNwLu0YO3HihPTo0UN++OEHmThxojRr1kzeffddGTVqlBQWFspvfvMb19sZOHCgbNu2TSZNmiSpqamSm5srq1atkn379vlu2ubOnSsjR46UrKwsef7556W0tFRmzJghXbp0kU2bNvnd3FVUVEhWVpZ06dJFXnjhhWvy3RjUTrt27RIR8V0ratLkyZNlypQpkpmZKePHj5cdO3bIjBkzZP369bJmzRoJDg6WoUOHyp/+9Cf56KOPZPDgwb7HlpaWypIlS2TUqFESGBgoIowZXHm5ubnSu3dvSUhIkCeeeELi4uJkz549fr9QPevtt9+W4uJiefjhhyUgIECmTp0q99xzj+zevVuCg4Or3c4nn3wiCxYskIkTJ0r9+vXllltukRkzZsj48eNlwIABcs8994iIyM033+x7zPr16yUvL0/69u0rImfGx0MPPSQ/+9nPZNy4cSIikp6eLiIi27Ztk65du0pMTIw8/vjjEhwcLLNmzZIePXrIZ599Jp07d/bbn4kTJ0pcXJxMnjzZN2737t3r+6Xbdce5Qf31r391AgMDncDAQOe2225zHn/8cWflypVOeXm533Ii4oSEhDg//PCDL9u8ebMjIs60adN82YMPPugkJyc7R44c8Xv8sGHDnNjYWKe0tNRxHMepqKhwysrK/JYpKChwkpKSnDFjxviy7OxsR0ScP/7xj86pU6ecoUOHOuHh4c7KlSt9y+zZs8cJDAx0nnnmGb/1ffvtt05QUJBf3r17d0dEnJkzZ7p9qYAqJkyY4Jx/+rCOsVdeecUREWfevHm+rLy83LntttucqKgop6ioyHEcx1m9erUjIs7q1av9Hn92LLz55puO45wZL2fHhqW4uNiJi4tzxo4d65cfPnzYiY2N9ctHjhzpiIjzxBNPXPTzB8735ptvOiLifPzxx05eXp6zf/9+55133nHi4+Od8PBw58cff3S6d+/udO/evcpjR44c6TRt2tQvExHnqaeeqrL+7Oxsx3EcJzc31wkJCXF69+7tnD592rfc66+/7oiI8+c//9lxHMeprKx0UlJSnIEDB/qtf8GCBY6IOJ9//rnjOIwZ1Bzt+mBZtGiRIyLO+vXrzWXOXgPi4+Od/Px8X7548WJHRJwlS5b4sqeeeqrKtkXEqVOnjrNt2za/PC8vr8o4O9eTTz5ZZVxGRkY6I0eOrLJs//79nZCQEGfXrl2+7ODBg050dLTTrVs3X3Z2HHfs2NHvfnPq1KmOiDiLFy82X4dr2Q37Mao77rhDvvzyS7n77rtl8+bNMnXqVMnKypKUlBT58MMP/ZbNzMz0zV5Fzsx8Y2JiZPfu3SJy5uNN77//vvziF78Qx3HkyJEjvv+ysrLk2LFjsnHjRhERCQwMlJCQEBE58xZdfn6+VFRUyE9/+lPfMucqLy+XwYMHy9KlS2XZsmW+LymJiCxcuFAqKytlyJAhftts0KCBtGjRospHs0JDQ2X06NE18wICCu0YW7ZsmTRo0MDv8+rBwcHy61//WkpKSuSzzz5ztY3w8HAJCQmRTz/91HzbedWqVVJYWCjDhw/3GxuBgYHSuXNn9WOL48ePd7UfgCYzM1MSEhKkcePGMmzYMImKipJFixZJSkpKjW7n448/lvLycnnkkUekTp2/X8rHjh0rMTEx8tFHH4nImY9+DB48WJYtWyYlJSW+5ebPny8pKSnSpUsXEWHM4Oo4++mLpUuXyqlTp6pddujQoVK3bl3fn7t27Soi4rsXq0737t2lbdu2rvZt2bJlvk+RVOf06dPy17/+Vfr37y9paWm+PDk5We6991754osvpKioyO8x48aN83s3Zvz48RIUFCTLli1ztY/Xihv2Y1QiIp06dZKFCxdKeXm5bN68WRYtWiQvv/yyDBo0SL755hvfgdmkSZMqj61bt67vRicvL08KCwtl9uzZMnv2bHVb537pfM6cOfLiiy/K9u3b/QZXs2bNqjzuueeek5KSElm+fHmVz/ru3LlTHMeRFi1aqNs8/23FlJQU30QH8IJ2jO3du1datGjhd0Mk8vdmkr1797raRmhoqDz//PPy6KOPSlJSkmRkZEi/fv1kxIgR0qBBAxE5MzZE/v55+fPFxMT4/TkoKEgaNWrkaj8AzZ/+9Cdp2bKlBAUFSVJSkrRq1arKsV8Tzo6bVq1a+eUhISGSlpbmN66GDh0qr7zyinz44Ydy7733SklJiSxbtsz3cRQRxgy8VVJS4jfZDQwMlISEBOnevbsMHDhQpkyZIi+//LL06NFD+vfvL/fee6+Ehob6reP8e7GzE4+L+a6Ddn9VncOHD8vGjRvl6aefvuCyeXl5UlpaWmUsipy5zlVWVsr+/fulXbt2vvz8+7aoqChJTk6+bv8dnRt6snFWSEiIdOrUSTp16iQtW7aU0aNHy7vvvitPPfWUiIjv86zncxxHRM68QyFyppFh5MiR6rJnPwc4b948GTVqlPTv31/+5V/+RRITEyUwMFCee+4532d7z5WVlSUrVqyQqVOnSo8ePSQsLMz3d5WVlRIQECDLly9X9zEqKsrvz+Hh4Rd6KYDLcjnHmPU51XO/vHrWI488Ir/4xS/kgw8+kJUrV8qTTz4pzz33nHzyySfyk5/8xDcm586d65uAnOv8tpLQ0FBPbghx4/nZz37ma6M6X0BAgO+6cS7tGK9JGRkZkpqaKgsWLJB7771XlixZIidOnJChQ4f6lmHMwEsvvPCCr2ZW5Mz3Zs+Wf7z33nuybt06WbJkiaxcuVLGjBkjL774oqxbt87vPuZC92LVcXttWr58uYSFhUnPnj1dPQ46JhvnOXuROHTo0EU/JiEhQaKjo+X06dOSmZlZ7bLvvfeepKWlycKFC/1urs5ObM6XkZEh//iP/yj9+vWTwYMHy6JFi3wn/fT0dHEcR5o1ayYtW7a86P0FrqSmTZvKli1bpLKy0u/mZPv27b6/F/n7b6nObdIRsd/5SE9Pl0cffVQeffRR2blzp3To0EFefPFFmTdvnu9jj4mJiRcck8CVUrduXfUjH27f3RP5+7jZsWOH30c3ysvLJTs7u8pxP2TIEHn11VelqKhI5s+fL6mpqZKRkeH7e8YMvDRixAjfR/ZEqt78Z2RkSEZGhjzzzDPy9ttvy3333SfvvPOOPPTQQ57tU3VfxP7oo4+kZ8+eVfZTe0xCQoJERETIjh07qvzd9u3bpU6dOtK4cWO/fOfOnX4TmZKSEjl06JDvy+jXmxv21xKrV69WZ8NnPy+nvR1mCQwMlIEDB8r7778vW7durfL3eXl5fsuK+M/E/+///k++/PJLc/2ZmZnyzjvvyIoVK+SBBx7w/QbqnnvukcDAQJkyZUqV5+I4jhw9evSinwPglb59+8rhw4dl/vz5vqyiokKmTZsmUVFR0r17dxE5c/MUGBgon3/+ud/jp0+f7vfn0tJSOXnypF+Wnp4u0dHRUlZWJiJn3hGMiYmRZ599Vv0c8LljErhS0tPTZfv27X7H3+bNm2XNmjWu15WZmSkhISHy2muv+Z3/33jjDTl27FiVz5oPHTpUysrKZM6cObJixQoZMmSI398zZuCltLQ0yczM9P13++23i8iZj0Cdf/9ytpXz7PncK2cb1M7/BdepU6dk1apV6vc1IiMjqywfGBgovXv3lsWLF/t9DConJ0fefvtt6dKlS5WPIc6ePdtvnM2YMUMqKiqkT58+l/ekaqkb9p2NSZMmSWlpqQwYMEBat24t5eXlsnbtWt9vfNx+kfo//uM/ZPXq1dK5c2cZO3astG3bVvLz82Xjxo3y8ccfS35+voiI9OvXTxYuXCgDBgyQu+66S7Kzs2XmzJnStm1bv88znq9///7y5ptvyogRIyQmJkZmzZol6enp8oc//EF++9vfyp49e6R///4SHR0t2dnZsmjRIhk3bpw89thjl/U6AZdr3LhxMmvWLBk1apR8/fXXkpqaKu+9956sWbNGXnnlFYmOjhYRkdjYWBk8eLBMmzZNAgICJD09XZYuXVrlH9n8/vvvpVevXjJkyBBp27atBAUFyaJFiyQnJ0eGDRsmImc+Xz5jxgx54IEH5NZbb5Vhw4ZJQkKC7Nu3Tz766CO5/fbb5fXXX7/irwVubGPGjJGXXnpJsrKy5MEHH5Tc3FyZOXOmtGvXrsoXSC8kISFBfvvb38qUKVPkzjvvlLvvvlt27Ngh06dPl06dOlX5B9VuvfVWad68ufz+97+XsrIyv49QiTBmcHXMmTNHpk+fLgMGDJD09HQpLi6W//zP/5SYmBjPf8sfHh4ubdu2lfnz50vLli2lXr160r59e8nLy5OioiJ1stGxY0f5+OOP5aWXXpKGDRtKs2bNpHPnzvKHP/xBVq1aJV26dJFf/epXEhQUJLNmzZKysjKZOnVqlfWUl5f7rmNnx22XLl3k7rvv9vQ5XzVXpQOrFli+fLkzZswYp3Xr1k5UVJQTEhLiNG/e3Jk0aZKTk5PjW05EnAkTJlR5fNOmTavUn+Xk5DgTJkxwGjdu7AQHBzsNGjRwevXq5cyePdu3TGVlpfPss886TZs2dUJDQ52f/OQnztKlS6tUH55bfXuu6dOnOyLiPPbYY77s/fffd7p06eJERkY6kZGRTuvWrZ0JEyY4O3bs8C3TvXt3p127dpf6cgF+rOpb6xjLyclxRo8e7dSvX98JCQlxbrrpJl+V7bny8vKcgQMHOhEREU7dunWdhx9+2Nm6datf9e2RI0ecCRMmOK1bt3YiIyOd2NhYp3Pnzs6CBQuqrG/16tVOVlaWExsb64SFhTnp6enOqFGjnA0bNviWGTlypBMZGXnpLwbg/L3SsroKT8dxnHnz5jlpaWlOSEiI06FDB2flypWXVH171uuvv+60bt3aCQ4OdpKSkpzx48c7BQUF6rZ///vfOyLiNG/e3Nw/xgwul5vq240bNzrDhw93mjRp4oSGhjqJiYlOv379/I43637IcaqOE6v6VruPcxzHWbt2rdOxY0cnJCTEt67HHnvMadu2rbr89u3bnW7dujnh4eGOiPjdB27cuNHJyspyoqKinIiICKdnz57O2rVr/R5/dhx/9tlnzrhx45y6des6UVFRzn333eccPXr0Qi/XNSvAcS7imzUAAADAda5t27bSr18/9R2Jy/XWW2/J6NGjZf369WaRxPXohv0YFQAAAHBWeXm5DB06tMp3mnB5mGwAAADghhcSEmK2g+LS3bBtVAAAAAC8xXc2AAAAAHiCdzYAAAAAeILJBgAAAABPMNkAAAAA4ImLbqMKCAjwcj+Ay1Ybvn50vY6TsLAwNX/ttdfUvLKyUs2tfyX5u+++U/Pw8HBzn5o0aaLm5eXlal5cXKzmMTExam49tyNHjpj7dC1gnFw8az+t3DruLfXq1VNz618Rrl+/vprHxsa62q6IyLfffqvmy5cvV3Nr/FyvasM4Ebl6YyUoSL89rKio8HS7AwYMUPMtW7ao+a5du7zcHRGxfwZZWVlqvmLFCi93p9a5mLHCOxsAAAAAPMFkAwAAAIAnmGwAAAAA8ASTDQAAAACeuOh/1O9a+UIfbly14Qt91+s4admypZrPmDFDza0vb1s/o7y8PFfrEbG/wHjixAk137t3r5q3adNGzQcNGqTmX3zxhblP1wLGycWz9tPtazh69GhXeVxcnJqHhoaqeWBgoLnt06dPu8oPHDig5s8++6yar169Ws3r1NF/l+n2S/RXS20YJyLej5Wa+jklJiaq+aRJk9S8b9++at60aVM1t47XQ4cOqfn69evV3Lo+iNjXm5tvvlnNrcKG7du3q/n//u//qvkf//hHNT98+LCa1zZ8QRwAAADAVcNkAwAAAIAnmGwAAAAA8ASTDQAAAACeYLIBAAAAwBO0UeG6URvaQ67XcTJs2DA1HzFihJpXVFSoeUlJiZqHh4e7ykXsn7fV5LNz5041z8zMVPPnn39ezV999VVzn64FjJOLZzWeWcf3ww8/rOa/+93v1NxqlyorK3OVV8d6ra0GK2v8fP3112p+xx131Mj+WK7W8VobxonI1Rsr3bp1U/MpU6aoeUZGhpoHBwereXFxsZoXFhaquTXmYmNj1dx63axWKxF7TFjHQmlpqZpbzzkqKkrNrYasr776Ss2ffvppNd+wYYOae402KgAAAABXDZMNAAAAAJ5gsgEAAADAE0w2AAAAAHiCyQYAAAAAT+hVGwBwDqulIzIyUs13796t5seOHVPz+Ph4NbfaO0REDh8+rObff/+9mjdt2lTNrSYQq/0EN47qmms06enpal63bl01z8/PV/OQkBBX262uschqxqlTR/9d46lTp1ytxy1rXysrK2tk/XBn9OjRam41HkVERKi51fZn/VytJjbruLSuQXl5eWpunb+thjkRu/3Qeg5um8Ks8W49t549e6p5nz591HzixIlqPmvWLDW39t+LJjbe2QAAAADgCSYbAAAAADzBZAMAAACAJ5hsAAAAAPAEkw0AAAAAnqCNCsAFHTx4UM0TEhLU/OjRo2putfu0atVKzVNTU819stpMrG0UFxerudUQ8uOPP5rbxo3BbSuL1YR25MgRNbeOyfr166t5TEyMmlsNPiJ225p13FttVE2aNDG34QatU1eH1XD2+OOPq7l1/rPaqMLCwtTcaiDMzs5Wc+vYt44/63i19se6bojY46ukpETNrdfU2ifr2mQ1vRUVFal5QUGBmv/zP/+zms+ePVvNvWidsvDOBgAAAABPMNkAAAAA4AkmGwAAAAA8wWQDAAAAgCeYbAAAAADwBG1UAC4oMDBQzSsqKtTcatdITExU8/DwcDUvLS019yk9Pd3VuqxGLWsbx48fN7eNG9v999+v5iNHjlRzq53Gaps5dOiQmkdGRqp5w4YN1VxEZNu2bWputf5YDXNWk84bb7yh5pMnT1bz/fv3qzm8NWLECDW3zqO7d+9W85ycHDW3GtGsxrWTJ0+qeb169dTcapGy8vXr16t5bm6umouINGrUSM2TkpLU3LouWtc/q73K7XXU+tlERUWp+a9//Ws1f/XVV9XcC7yzAQAAAMATTDYAAAAAeILJBgAAAABPMNkAAAAA4AkmGwAAAAA8QRsVgAvKy8tT88rKSlfriYiIUHOr1ePll1821zVp0iQ137dvn5qnpaWpeVlZmZpb7Tu4cWRmZqr5888/r+ZW847V1JOamupqPQUFBWp+7NgxNRexG+Cs3HEcNS8vL1fzO++8U81btmyp5l27dlVzeKtTp05qbh2bVnNSgwYN1DwmJqZG1m+1VO3cudPV+q3zt3UdELGfgzXurDFhNb0FBem33NZzsNZvtdhZbVQPPPCAmtNGBQAAAOCax2QDAAAAgCeYbAAAAADwBJMNAAAAAJ5gsgEAAADAE7RRAbig/fv3q7nVUnX48GE1t1o6rBaNH374wdyn8PBwNS8pKVHz7777ztV6gKFDh6p5YWGhmh8/flzNQ0ND1byiokLNw8LC1Dw4OFjNmzRpouYiIrt27VLzEydOqPmpU6fU3GqMO3jwoJonJCSoeUZGhpqvW7dOzVEzGjVqpOZW01JcXJyaHzhwQM0PHTqk5lbb3969e9XcOs6s5sP4+Hg1t56vdXxXt+3k5GQ1t14jq1HLapey2qis66jVamWNaatB7ErinQ0AAAAAnmCyAQAAAMATTDYAAAAAeILJBgAAAABPMNkAAAAA4AnaqGqhHj16qHndunXVfNGiRR7ujfcCAgJcLe84jkd7Akt+fr6aHzt2TM0jIyPVPDo6Ws0jIiLU3Gq7ErGbPax1nT592lyXxmqvwo2jupYnN6xjz2qPsVqnrPVY41DEbtixcres87f1HFJTU9WcNipvJSUlqbl17FjNgdaYsM7VRUVFat62bVs137Ztm5q7vaZYY8VqKxSxz/nWsWwd+9a+WmOuVatWat64cWM1b9iwoZpbDV/WmLuSeGcDAAAAgCeYbAAAAADwBJMNAAAAAJ5gsgEAAADAE0w2AAAAAHiCNiqPWa0BMTEx5mOaNWum5mFhYWputUzk5ORcYO/8Wc0KVm61Qrlti6Jd6tp19OhRNY+NjVVzq33Haun46quvzG1bLSd16ui/Q7HaSSorK9Xc7fjB9cc6f1vHknXslZWVqbl1TrfWHxISouYVFRVqLuL+/Hry5Ek1d9vIY8nIyFDzd955x9V6oEtLS1Pz5s2bq3lubq6aWy1V1rG2e/duNbeOJ2ts9erVS82tpqX09HQ1t4778PBwNRexG7Ks8WtdO6x7sj179qh5fHy8mlvj3Wr+srZbWFio5laz2L59+9T8cvDOBgAAAABPMNkAAAAA4AkmGwAAAAA8wWQDAAAAgCeYbAAAAADwxA3fRuW2acmSnJys5lYzRKtWrcx1ffPNN2p+8803q/mtt96q5suXLze3oampdimL9Vo/9thjaj537lw1P3z4cI3sDy7fli1b1Pwf/uEfXK3HavGpjtVsZTV4BAXpp7tDhw653jauL9a5yWp3sY6Z0NBQNS8vL3eVW0pLS9W8unO01dpmNcBZr4XVyBMZGanmVqOWdb1CzbAahqxGIiu3GpKsexrrGLSOD6vVyjr+rO0WFBSouXW+t8aQiH0dshqsrGuNdY+Sn5+v5nFxcWper149V+ux2rGsn43V5EUbFQAAAIBrBpMNAAAAAJ5gsgEAAADAE0w2AAAAAHiCyQYAAAAAT1x0G5XVEOCW9W1/q7nCa26blqwWqWbNmql5y5Yt1by6BpyUlBQ1t5oJSkpK1Lxt27Zqvn//fjUvLi4290ljHRNWM1evXr3UvEWLFmretWtXNX/33XcvYu9wJezevVvN+/btq+ZWW8bJkyddb9tqo7JadiwHDx50vW1cXxo3bqzmVguNdd2wWmWsJidrPLi9HlbX5mY9B+v8bTVkhYWFqbnVNmRdl+rXr6/mqBnWvYjVLmW1lVltUdu3b1fz7OxsNbfugQ4cOKDmt9xyi5ofOXJEzSMiItTcel5We5WISGpqqppbY8VqhYqJiVFz6x4uNzdXza3X2hpD1j2cNUYzMzPVfPXq1Wp+OXhnAwAAAIAnmGwAAAAA8ASTDQAAAACeYLIBAAAAwBNMNgAAAAB44qLbqKz2DasFw/r2vtW+cbVYrQHt27dXc6spymr82LJli5pHRUWZ+2Q1B/z4449qftttt6l5gwYN1Hzr1q1qbv1srFYR62dstWBZr53VzDVmzBg1p42q9rB+dlaTiSUkJMT1tq2GH2vb1jksLy/P9bZxfbnpppvU3DqWrHOf1YRmLW9dP4ODg9X8+PHjrrYrYre2RUdHq7nba73bpi3ruoSacfToUTXftGmTmlsNm1bj0dy5c9V8x44dat6uXTs1t46bbdu2qbnVplVYWKjmFqtVTURkzZo1am7dk1kyMjLU3LqHy8nJUXPr/NO8eXM1t8a61Wr1ww8/qLkXeGcDAAAAgCeYbAAAAADwBJMNAAAAAJ5gsgEAAADAE0w2AAAAAHjiotuoLLWtXcpqDbDaRtq0aaPm1rf6rdxqbEpOTlbzoCD7pbdaP6zGns2bN6t5amqqmnfr1k3NDxw4oOahoaGu1m81qViNKVbLRGJioqv14MrbuXOnmickJKi51ZRS3XiwnDp1Ss2t48M6V1kNP7hxWA1JJ0+eVHPrHGe1u1jHWNOmTV0tb13frLEgYrfvnD59Ws2tFqK3335bze+88041t8bhwYMH1dxqxyouLlZz6D788ENXuSUtLU3Nd+/ereb//u//ruZW65TVqGRdC6x7L2uMWvcn1Y0V6zxgNV5Z+2o1HFpNb1YjWOPGjdXcem5Wq5X12l1JvLMBAAAAwBNMNgAAAAB4gskGAAAAAE8w2QAAAADgCSYbAAAAADxx2W1U7du3V/PDhw+rufVtfKuJIioqylVutQlYrQH5+flqbrXpHDlyRM2t1gCL1fAkYreHWGJiYtTcav2wXgurEcFqwSooKFBzq/XHaoH45JNP1DwzM1PN3b7W8I7VmrNt2zY1r1evnppbx1h1rHFSWlqq5uXl5WpO2w3q1q2r5laLi9WU9z//8z9qnpSUpOYRERFqbrUbWufu6lohrQYrq8UnNjZWza0x/fOf/1zNrWu620aeDRs2qDm8ZbVOWawGJuuebOPGja7Wb90zBQQEqLl1XreuAyLurylW+1xFRYWaWw1fKSkp5j5p9uzZ42r52oB3NgAAAAB4gskGAAAAAE8w2QAAAADgCSYbAAAAADzBZAMAAACAJ5hsAAAAAPDERVffWpVgs2fPVvPg4GA1f/fdd9V8y5Ytam7Vq1rVYtbyVg2fVZtmVQxaVb9/+9vf1Lxhw4autitiVwNWV2+oqV+/vppbdaXfffedmlt1pZbc3Fw1t6rorGq8W2+9Vc2t1we1h3V8W+eRwMBA19uwqpStGlOrLnDXrl2ut43ri1VNa11nrDrZb7/91tX669Rx9/s+6xpQ3TnReg4W69pt1VNblafx8fFqblXuNm/eXM2pvq0Z1jnZOgat3DrvWlX7p0+fVnPr+LCql61jPDw8XM2t/bTy6raRnJys5tY/kWBdz6x7oKKiInOf3LC2eynnjZrGOxsAAAAAPMFkAwAAAIAnmGwAAAAA8ASTDQAAAACeYLIBAAAAwBMX3UZlfeu+R48eah4XF6fmw4YNU/NBgwap+dGjR9V8zZo1am41DViNSlZTwrFjx9TcaiGx1l9eXq7moaGhai5it0VZLSFW00B1jVcaqw3EaiGxWkW6d++u5tZ+tmjRwlVuHROoPRITE13l1TWEWKzWs5SUFDW3WqpKSkpcbxvXF6ut0Gpyss6t1vm+UaNGrpa32hCt7VbXKmO1ClnPzbomWq2Bhw4dUvObb75Zza2xbrUToWZYx4j183bLOs6sY9a6B7Ialaz9LCsrU/NLaWCytmHl1nO27h+t84x1z+dWbWidsvDOBgAAAABPMNkAAAAA4AkmGwAAAAA8wWQDAAAAgCeYbAAAAADwxEW3UcXGxqr5zJkz1Xzp0qVqPn36dDV32/rRu3dvNbcaM6zGAquhxmJ9279z585qbrUShIeHm9uwmryshhKr3cNqILBapKzX2nrtSktL1Xzt2rVqfuLECTWfP3++mlstJ7QH1X7WeLaazaxjrDrW8Wo1h1hNcm5b23D9sc7HVgOgdQ7Kz89Xc6ut0DqHWqxz+qUcw9b1xBqLVjvg8uXL1TwrK8vV/oSFhblaHrWLdW5326BmNbRZ1xRrjFpNUdWxxoTF2qd9+/apeXJysut9ul7wzgYAAAAATzDZAAAAAOAJJhsAAAAAPMFkAwAAAIAnmGwAAAAA8MRFf/V+69atan7XXXep+dChQ9V88eLFam41xXz//fdqvnPnTjUvLi5W89zcXDU/evSomluNCFbTjdWcdPz4cTUvKChQcxF7X63nZjWaFBUVqbnVAlHb9OnTR83/6Z/+6QrvCdyyWjqsYy80NNT1NgoLC9U8MDBQzePj49W8cePGar5r1y7X+4Rrk9WkYx2XOTk5am4dkykpKWr+t7/9Tc2tZiarefBSREREqLl1zW3ZsqWar1q1Ss1rqoEL1wa3bWLWedpti5TbNirr2lTdPrltAk1LS1Nza19r6p7MbfPXlcQ7GwAAAAA8wWQDAAAAgCeYbAAAAADwBJMNAAAAAJ5gsgEAAADAExfdRmV9s7+yslLN58+f7yq31n/33XerudWM0aFDBzUfMmSImlstAxbr2/5WS5XValUdqxnFauyyWkWsdghr/dbPwMqthgOrccF67ayWoLKyMjWvX7++mr/00ktqjisvPDxczd02h1THWld0dLSaW01Be/bscb1tXF+sFhrr3Prdd9+puXX9sdZvNeNYy1us67CI+4Yaa9upqalqbrUk5ufnq7k11k+ePKnmuDZYY8ViNb1VdyxrrOPVun+orpnJGivWtcZtu5R1v2m1orpl7X9twDsbAAAAADzBZAMAAACAJ5hsAAAAAPAEkw0AAAAAnmCyAQAAAMATF13F5LYhwC1r/R988IGn2wVQ806dOuVqeashrTpW+4l1LrH26VIa43B9sVpcrAbATZs2uVreamCymnSsY9ha/lJaaKwGRaslqF69emqem5ur5lZLldWSSBvVtc06Zg8ePKjm1jFrndet4/L48eNqbrVRVTdWrGOztLRUza0xZLVOWa+Rtd3rCe9sAAAAAPAEkw0AAAAAnmCyAQAAAMATTDYAAAAAeILJBgAAAABPXHQbFQBcLKshxGrluRTWuhzH8XzbuL5YbWjWMXPo0CE1T0tLU3OrCc3tdmuyFdJq5bEadqxxZe3T0aNH1Tw9PV3NGZ+1i3UcWDp37qzm1nFz+PBhNbdapxITE9W8JhuerDFx4sQJNXfbMme9ptHR0WpuvRZW05bb7V5JjG4AAAAAnmCyAQAAAMATTDYAAAAAeILJBgAAAABPMNkAAAAA4AnaqADUuIKCAjW32jJ+/PFH19uwmnwqKirU3GpFAaxWGaudxjq+O3XqpOZu22Os7QYHB7taXsRuorFyt/sUHx+v5vv371fzVq1aqTltVNc2qy2qpKREzbOzs9U8JSVFzZOTk13tj3V8l5aWmo+xxldsbKyal5eXq7l1rTl58qSah4eHq7k1VrZs2aLmtRmjGwAAAIAnmGwAAAAA8ASTDQAAAACeYLIBAAAAwBNMNgAAAAB4gjYqABdkNcVUVlaq+dq1a9W8X79+rrYbExNj/p3VmhMaGqrmRUVFrnLcOIKC9Euh1U6Tl5en5vn5+WpuHWPW+LGadCzVtVG5Ze2T9ZytcWgtb73WtFFd26xztdW01KBBA1frsY6P6OhoNd+3b5+ab9++Xc1F7GM5Li5Oza1xarVRpaammtvWREREuFq+NmN0AwAAAPAEkw0AAAAAnmCyAQAAAMATTDYAAAAAeILJBgAAAABP0EYF4IKsdg3Lli1b1Nxt+47VXFMda10VFRVqXlJS4nobuL5Yx3dBQYGaFxYWqvnu3bvV/I477lBzq+3KasVxOw5F7KYqq93HGj/x8fFqvnPnTjXv1q2bmlvPrbrmOXjHOj7cHmtW+1j9+vXVPC0tTc2t48xa/9GjR13lkZGRai4i0qZNGzU/efKkmltjxXrO1nrKysrU3GrastRkK11N450NAAAAAJ5gsgEAAADAE0w2AAAAAHiCyQYAAAAATzDZAAAAAOAJ2qgA1Lh9+/apeX5+vpo3aNBAzY8fP25uw2onseTk5LhaHrA0a9ZMzX/zm9+oudXsU69ePTUPDw9Xc6udxmp5E7Fb2KzcasiyWtt27Nih5tZzsBp82rdvr+a4thUXF6t5dna2mlvNTFbTUmhoqJqnpKSouTXmROzrk3UsW81qu3btUvNjx46peePGjdXcGqOWS2mru1J4ZwMAAACAJ5hsAAAAAPAEkw0AAAAAnmCyAQAAAMATTDYAAAAAeII2KgA1zmqRCgkJUXOrZadRo0bmNhITE9U8KipKzffv32+uCzc26/izGnDi4uJcrf+RRx5xuUfXvvj4eDUPCtJvO8LCwrzcHRhqqsHIan+yGptuuukmNT906JCaW22CCQkJan7ixAk1Ly8vV3MRkcLCQjW3GrIiIiLU3LpuWeeZ2NhYNbeavK5FvLMBAAAAwBNMNgAAAAB4gskGAAAAAE8w2QAAAADgCSYbAAAAADxBGxWAGmc1nHz66adqfsstt6j5rl27zG3Mnz9fza12EmvbwFdffeVq+S+//NLV8lYDU0VFhav1XE3BwcFqfurUKTVfsmSJmqelpan5hg0bLm3HUCvMmTNHzdu0aaPm1pjo0KGDmlsNcPXq1VPzunXrqrnVjiUiUllZqeZum62sFrsDBw6o+TfffKPmmzdvVnOLtf+1Ae9sAAAAAPAEkw0AAAAAnmCyAQAAAMATTDYAAAAAeILJBgAAAABPBDhWbQwAAAAAXAbe2QAAAADgCSYbAAAAADzBZAMAAACAJ5hsAAAAAPAEkw0AAAAAnmCyAQAAAMATTDYAAAAAeILJBgAAAABPMNkAAAAA4In/B75enwHI9Cj/AAAAAElFTkSuQmCC"},"metadata":{}}],"execution_count":6},{"id":"aa677cd3-8158-4ee9-95fe-aecb5974c364","cell_type":"code","source":"class FashionClassifier(nn.Module):\n    def __init__(self, channels:int=1, image_size:int=28):\n        super(FashionClassifier, self).__init__()\n        self.channels = channels\n        self.image_size = image_size\n\n        self.conv1 = nn.Conv2d(self.channels, out_channels=128, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1)\n       \n        self.flatten = nn.Flatten()\n    \n        self.fc1 = nn.Linear(64 * (image_size // 2) * (image_size // 2), 128)\n        self.fc2 = nn.Linear(128, 10)\n\n        self.dropout1 = nn.Dropout(0.25)\n        self.dropout2 = nn.Dropout(0.5)\n        \n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        \n        x = self.flatten(x)\n        \n        x = self.dropout1(x)\n        \n        x = F.relu(self.fc1(x))\n        \n        x = self.dropout2(x)\n        \n        x = self.fc2(x)\n\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T11:56:09.003106Z","iopub.execute_input":"2024-12-17T11:56:09.004054Z","iopub.status.idle":"2024-12-17T11:56:09.011390Z","shell.execute_reply.started":"2024-12-17T11:56:09.004015Z","shell.execute_reply":"2024-12-17T11:56:09.010449Z"}},"outputs":[],"execution_count":7},{"id":"874695aa-8761-4096-9faa-30c7717fd438","cell_type":"code","source":"from torchsummary import summary\n\nfashion_classifier = FashionClassifier().to(DEVICE)\nprint(fashion_classifier)\nsummary(fashion_classifier, (1, 28, 28))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T11:56:10.170602Z","iopub.execute_input":"2024-12-17T11:56:10.170958Z","iopub.status.idle":"2024-12-17T11:56:10.470678Z","shell.execute_reply.started":"2024-12-17T11:56:10.170926Z","shell.execute_reply":"2024-12-17T11:56:10.469700Z"}},"outputs":[{"name":"stdout","text":"FashionClassifier(\n  (conv1): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv2): Conv2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (fc1): Linear(in_features=12544, out_features=128, bias=True)\n  (fc2): Linear(in_features=128, out_features=10, bias=True)\n  (dropout1): Dropout(p=0.25, inplace=False)\n  (dropout2): Dropout(p=0.5, inplace=False)\n)\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1          [-1, 128, 28, 28]           1,280\n            Conv2d-2           [-1, 64, 14, 14]          73,792\n           Flatten-3                [-1, 12544]               0\n           Dropout-4                [-1, 12544]               0\n            Linear-5                  [-1, 128]       1,605,760\n           Dropout-6                  [-1, 128]               0\n            Linear-7                   [-1, 10]           1,290\n================================================================\nTotal params: 1,682,122\nTrainable params: 1,682,122\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 1.05\nParams size (MB): 6.42\nEstimated Total Size (MB): 7.47\n----------------------------------------------------------------\n","output_type":"stream"}],"execution_count":8},{"id":"0ef5db83-a69c-4754-901e-b942a46001ce","cell_type":"code","source":"class ClassifierModule(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n\n        self.net = FashionClassifier()\n        self.criterion = nn.CrossEntropyLoss()\n        \n        self.train_acc = Accuracy(task=\"multiclass\", num_classes=len(labels))\n        self.val_acc = Accuracy(task=\"multiclass\", num_classes=len(labels))\n        self.test_acc = Accuracy(task=\"multiclass\", num_classes=len(labels))\n\n        self.train_loss = MeanMetric()\n        self.val_loss = MeanMetric()\n        self.test_loss = MeanMetric()\n\n        self.val_acc_best = MaxMetric()\n\n    def forward(self, x): \n        return self.net(x)\n\n    def model_step(\n        self, batch: Tuple[torch.Tensor, torch.Tensor]\n    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Perform a single model step on a batch of data.\n\n        :param batch: A batch of data (a tuple) containing the input tensor of images and target labels.\n\n        :return: A tuple containing (in order):\n            - A tensor of losses.\n            - A tensor of predictions.\n            - A tensor of target labels.\n        \"\"\"\n        x, y = batch\n        logits = self.forward(x)\n        loss = F.cross_entropy(logits.float(), y)\n        preds = torch.argmax(logits, dim=1)\n\n        return loss, preds, y\n        \n    def on_train_start(self) -> None:\n        \"\"\"Lightning hook that is called when training begins.\"\"\"\n        # by default lightning executes validation step sanity checks before training starts,\n        # so it's worth to make sure validation metrics don't store results from these checks\n        self.val_loss.reset()\n        self.val_acc.reset()\n        self.val_acc_best.reset()\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        loss, preds, targets = self.model_step(batch)\n        \n        self.train_loss(loss)\n        self.train_acc(preds, targets)\n        self.log(\"train/loss\", self.train_loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log(\"train/acc\", self.train_acc, on_step=False, on_epoch=True, prog_bar=True)\n\n        return loss\n\n    def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> None:\n        x, y = batch\n        loss, preds, targets = self.model_step(batch)\n\n        self.val_loss(loss)\n        self.val_acc(preds, targets)\n        \n        self.log(\"val/loss\", self.val_loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log(\"val/acc\", self.val_acc, on_step=False, on_epoch=True, prog_bar=True)\n\n    def on_validation_epoch_end(self) -> None:\n        \"Lightning hook that is called when a validation epoch ends.\"\n        acc = self.val_acc.compute() \n        self.val_acc_best(acc)  \n        self.log(\"val/acc_best\", self.val_acc_best.compute(), sync_dist=True, prog_bar=True)\n\n    def test_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> None:\n        x, y = batch\n        loss, preds, targets = self.model_step(batch)\n        \n        self.test_loss(loss)\n        self.test_acc(preds, targets)\n        \n        self.log(\"test/loss\", self.test_loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log(\"test/acc\", self.test_acc, on_step=False, on_epoch=True, prog_bar=True)\n\n\n    def configure_optimizers(self):\n        optimizer=torch.optim.Adam(params=self.parameters(), lr=0.001)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.1, patience=10)\n      \n        return {\n           \"optimizer\": optimizer,\n           \"lr_scheduler\": {\n               \"scheduler\": scheduler,\n               \"monitor\": \"val/loss\",\n           },\n        }    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T11:56:12.601677Z","iopub.execute_input":"2024-12-17T11:56:12.602039Z","iopub.status.idle":"2024-12-17T11:56:12.616140Z","shell.execute_reply.started":"2024-12-17T11:56:12.602005Z","shell.execute_reply":"2024-12-17T11:56:12.615102Z"}},"outputs":[],"execution_count":9},{"id":"dd1fa1fa-24fb-45bb-9703-3710d0a8f6ff","cell_type":"code","source":"# Get the current date and time\ncurrent_datetime = datetime.now(timezone(\"UTC\")).strftime(\"%Y-%m-%d_%H-%M-%S\")\n\n# Initialize the WandbLogger\nlogger = WandbLogger(project=\"puvae\", name=f\"run-{current_datetime}\", tags= [\"fashion\", \"classifier\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T11:56:19.848648Z","iopub.execute_input":"2024-12-17T11:56:19.849003Z","iopub.status.idle":"2024-12-17T11:56:19.856954Z","shell.execute_reply.started":"2024-12-17T11:56:19.848973Z","shell.execute_reply":"2024-12-17T11:56:19.856084Z"}},"outputs":[],"execution_count":10},{"id":"eb93cb85-9c00-4287-b880-632be86e7d1e","cell_type":"code","source":"checkpoint_callback = ModelCheckpoint(\n   dirpath=\"checkpoints\",\n   monitor=\"val/loss\",\n   filename=\"classifier-{epoch:02d}-{val_loss:.2f}-{val_acc:.2f}\",\n   save_top_k=3,\n   mode=\"min\",\n)\n\nearly_stopping = EarlyStopping(monitor=\"val/loss\", patience=5, mode=\"min\", verbose=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T11:56:20.981777Z","iopub.execute_input":"2024-12-17T11:56:20.982835Z","iopub.status.idle":"2024-12-17T11:56:20.988782Z","shell.execute_reply.started":"2024-12-17T11:56:20.982776Z","shell.execute_reply":"2024-12-17T11:56:20.987838Z"}},"outputs":[],"execution_count":11},{"id":"75bb52f9-bfb1-4416-a3d9-fd7c70141d90","cell_type":"code","source":"classifier = ClassifierModule().to(DEVICE)\ntrainer = pl.Trainer(\n    max_epochs=100,\n    logger=logger,\n    callbacks = [checkpoint_callback, early_stopping],\n    accumulate_grad_batches=2,\n    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n    devices=\"auto\",\n)\n\ntrainer.fit(classifier, train_dataloaders=train_loader, val_dataloaders=test_loader)\n\nlogger.experiment.save(\"checkpoints/*.ckpt\")\nwandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T11:56:26.097629Z","iopub.execute_input":"2024-12-17T11:56:26.097998Z","iopub.status.idle":"2024-12-17T11:59:00.148423Z","shell.execute_reply.started":"2024-12-17T11:56:26.097965Z","shell.execute_reply":"2024-12-17T11:59:00.147675Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>./wandb/run-20241217_115626-9fo2zzc8</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/iai-uet-vnu/puvae/runs/9fo2zzc8' target=\"_blank\">run-2024-12-17_11-56-19</a></strong> to <a href='https://wandb.ai/iai-uet-vnu/puvae' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/iai-uet-vnu/puvae' target=\"_blank\">https://wandb.ai/iai-uet-vnu/puvae</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/iai-uet-vnu/puvae/runs/9fo2zzc8' target=\"_blank\">https://wandb.ai/iai-uet-vnu/puvae/runs/9fo2zzc8</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /kaggle/working/checkpoints exists and is not empty.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6d1538d78fc4220a4e7a44cfaa61dc4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Symlinked 10 files into the W&B run directory, call wandb.save again to sync new files.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='1.283 MB of 215.150 MB uploaded\\r'), FloatProgress(value=0.005962784706437211, max…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <style>\n        .wandb-row {\n            display: flex;\n            flex-direction: row;\n            flex-wrap: wrap;\n            justify-content: flex-start;\n            width: 100%;\n        }\n        .wandb-col {\n            display: flex;\n            flex-direction: column;\n            flex-basis: 100%;\n            flex: 1;\n            padding: 10px;\n        }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇██</td></tr><tr><td>train/acc</td><td>▁▄▅▆▆▇▇▇▇████</td></tr><tr><td>train/loss</td><td>█▅▄▃▃▂▂▂▂▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇██</td></tr><tr><td>val/acc</td><td>▁▄▅▆▆▇▇▇▇▇▇██</td></tr><tr><td>val/acc_best</td><td>▁▄▅▆▆▇▇▇▇▇▇██</td></tr><tr><td>val/loss</td><td>█▄▃▂▂▂▂▁▁▂▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>12</td></tr><tr><td>train/acc</td><td>0.9419</td></tr><tr><td>train/loss</td><td>0.14975</td></tr><tr><td>trainer/global_step</td><td>6096</td></tr><tr><td>val/acc</td><td>0.922</td></tr><tr><td>val/acc_best</td><td>0.9225</td></tr><tr><td>val/loss</td><td>0.24293</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">run-2024-12-17_11-56-19</strong> at: <a href='https://wandb.ai/iai-uet-vnu/puvae/runs/9fo2zzc8' target=\"_blank\">https://wandb.ai/iai-uet-vnu/puvae/runs/9fo2zzc8</a><br/> View project at: <a href='https://wandb.ai/iai-uet-vnu/puvae' target=\"_blank\">https://wandb.ai/iai-uet-vnu/puvae</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20241217_115626-9fo2zzc8/logs</code>"},"metadata":{}}],"execution_count":12},{"id":"2cdaf71e-1985-448b-a2a2-fd65b5f7f365","cell_type":"markdown","source":"# VAE Architecture","metadata":{}},{"id":"655e030e-8a1d-4de0-bbaf-8f4b1fd3fcda","cell_type":"code","source":"from abc import abstractmethod\n\n\nclass BaseVAE(nn.Module):\n\n    def __init__(self) -> None:\n        super().__init__()\n\n    def encode(self, img: Tensor) -> Tuple[Tensor, Tensor]:\n        raise NotImplementedError\n\n    def decode(self, z: Tensor) -> Tensor:\n        raise NotImplementedError\n\n    @torch.no_grad()\n    def sample(self, n_samples: int, device: str = torch.device(\"cpu\")) -> Tensor:\n        \"\"\"_summary_\n        Samples from the latent space and return the corresponding image space map.\n        Args:\n            n_samples (int): Number of samples\n            device (str, optional): Device to run the model. Defaults to torch.device(\"cpu\").\n\n        Returns:\n            Tensor: _description_\n        \"\"\"\n\n        z = torch.randn(n_samples, self.latent_dims[0], self.latent_dims[1],\n                        self.latent_dims[2], device=device)\n        return self.decode(z)\n\n    @abstractmethod\n    def forward(self, img: Tensor) -> Tuple[Tensor, Dict[str, Tensor]]:\n        pass\n\n    @abstractmethod\n    def loss_function(self, img: Tensor, recons_img: Tensor,\n                      **kwargs) -> Tensor:\n        pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T16:23:50.441434Z","iopub.execute_input":"2024-12-17T16:23:50.441779Z","iopub.status.idle":"2024-12-17T16:23:50.450088Z","shell.execute_reply.started":"2024-12-17T16:23:50.441748Z","shell.execute_reply":"2024-12-17T16:23:50.449234Z"}},"outputs":[],"execution_count":224},{"id":"570cb1cd-9b14-4d75-98dc-a72a01616bcb","cell_type":"code","source":"class ConditionalVAE(BaseVAE):\n    def __init__(self,\n                 in_channels: int,\n                 num_classes: int,\n                 latent_dim: int,\n                 hidden_dims: List = None,\n                 img_size:int = 64,\n                 ) -> None:\n        super(ConditionalVAE, self).__init__()\n        \n        self.latent_dim = latent_dim\n        self.img_size = img_size\n\n        self.embed_class = nn.Linear(num_classes, img_size * img_size)\n        self.embed_data = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n\n        modules = []\n        if hidden_dims is None:\n            hidden_dims = [32, 64, 128, 256, 512]\n\n        in_channels += 1 # To account for the extra label channel\n        # Build Encoder\n        for h_dim in hidden_dims:\n            modules.append(\n                nn.Sequential(\n                    nn.Conv2d(in_channels, out_channels=h_dim,\n                              kernel_size= 3, stride= 2, padding  = 1),\n                    nn.BatchNorm2d(h_dim),\n                    nn.LeakyReLU())\n            )\n            in_channels = h_dim\n\n        self.encoder = nn.Sequential(*modules) \n        self.fc_mu = nn.Linear(hidden_dims[-1], latent_dim)\n        self.fc_var = nn.Linear(hidden_dims[-1], latent_dim)\n\n\n        # Build Decoder\n        modules = []\n\n        self.decoder_input = nn.Linear(latent_dim + num_classes, hidden_dims[-1] * 4)\n\n        hidden_dims.reverse()\n\n        for i in range(len(hidden_dims) - 1):\n            modules.append(\n                nn.Sequential(\n                    nn.ConvTranspose2d(hidden_dims[i],\n                                       hidden_dims[i + 1],\n                                       kernel_size=3,\n                                       stride = 2,\n                                       padding=1,\n                                       output_padding=1\n                                       ),\n                    nn.BatchNorm2d(hidden_dims[i + 1]),\n                    nn.LeakyReLU())\n            )\n\n\n\n        self.decoder = nn.Sequential(*modules)\n\n        self.final_layer = nn.Sequential(\n                            nn.ConvTranspose2d(hidden_dims[-1],\n                                               hidden_dims[-1],\n                                               kernel_size=3,\n                                               stride=1,\n                                               padding=1,\n                                               output_padding=1),\n                            nn.BatchNorm2d(hidden_dims[-1]),\n                            nn.LeakyReLU(),\n                            nn.Conv2d(hidden_dims[-1], out_channels= 1,\n                                      kernel_size= 3, padding= 1),\n                            nn.Tanh())\n\n    def encode(self, input: Tensor) -> List[Tensor]:\n        \"\"\"\n        Encodes the input by passing through the encoder network\n        and returns the latent codes.\n        :param input: (Tensor) Input tensor to encoder [N x C x H x W]\n        :return: (Tensor) List of latent codes\n        \"\"\"\n        result = self.encoder(input)\n        result = torch.flatten(result, start_dim=1)\n\n        # Split the result into mu and var components\n        # of the latent Gaussian distribution\n        \n        mu = self.fc_mu(result)\n        log_var = self.fc_var(result)\n\n        return [mu, log_var]\n\n    def decode(self, z: Tensor) -> Tensor:\n        result = self.decoder_input(z) # [64, 2048]\n        print(f\"Decoder input{result.shape}\")\n        result = result.view(-1, 512, 2, 2) # [64, 512, 2, 2]\n        print(f\"Decoder view{result.shape}\")\n        result = self.decoder(result) # [64, 128, 16, 16]\n        print(f\"Decoder {result.shape}\")\n        result = self.final_layer(result) # [64, 1, 32, 32]\n        return result\n\n    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:\n        \"\"\"\n        Will a single z be enough ti compute the expectation\n        for the loss??\n        :param mu: (Tensor) Mean of the latent Gaussian\n        :param logvar: (Tensor) Standard deviation of the latent Gaussian\n        :return:\n        \"\"\"\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return eps * std + mu\n\n    def forward(self, input, y) -> List[Tensor]:\n        y = y.float()\n        embedded_class = self.embed_class(y)\n        embedded_class = embedded_class.view(-1, self.img_size, self.img_size).unsqueeze(1)\n        embedded_input = self.embed_data(input)\n\n        x = torch.cat([embedded_input, embedded_class], dim = 1)\n        mu, log_var = self.encode(x)\n\n        z = self.reparameterize(mu, log_var)\n\n        z = torch.cat([z, y], dim = 1)\n\n        return  self.decode(z), input, mu, log_var\n\n    def loss_function(self,\n                     recons,\n                     input,\n                     mu, log_var) -> dict:\n\n        kld_weight = 0.0050  # Account for the minibatch samples from the dataset\n        print(recons.shape)\n        print(input.shape)\n        recons_loss =F.mse_loss(recons, input)\n\n        kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n\n        loss = recons_loss + kld_weight * kld_loss\n        return loss, recons_loss, kld_loss\n\n    def sample(self,\n               num_samples:int,\n               current_device: int,\n               **kwargs) -> Tensor:\n        \"\"\"\n        Samples from the latent space and return the corresponding\n        image space map.\n        :param num_samples: (Int) Number of samples\n        :param current_device: (Int) Device to run the model\n        :return: (Tensor)\n        \"\"\"\n        y = kwargs['labels'].float()\n        z = torch.randn(num_samples,\n                        self.latent_dim)\n\n        z = z.to(current_device)\n\n        z = torch.cat([z, y], dim=1)\n        samples = self.decode(z)\n        return samples\n\n    def generate(self, x: Tensor, **kwargs) -> Tensor:\n        \"\"\"\n        Given an input image x, returns the reconstructed image\n        :param x: (Tensor) [B x C x H x W]\n        :return: (Tensor) [B x C x H x W]\n        \"\"\"\n\n        return self.forward(x, **kwargs)[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T17:56:17.422395Z","iopub.execute_input":"2024-12-17T17:56:17.422755Z","iopub.status.idle":"2024-12-17T17:56:17.441185Z","shell.execute_reply.started":"2024-12-17T17:56:17.422722Z","shell.execute_reply":"2024-12-17T17:56:17.440313Z"}},"outputs":[],"execution_count":478},{"id":"0ab8c620-ed1a-49b9-a61e-492c84b94045","cell_type":"code","source":"from torchvision.utils import make_grid\n\nclass cVAEModule(pl.LightningModule):\n    def __init__(self, kl_weight:int=0.0025, rc_weight:int=1, ce_weight: int=10.0):\n        super().__init__()\n        self.net = ConditionalVAE(in_channels=1,img_size= 28, num_classes=10, latent_dim=32)\n        \n        self.kl_weight = kl_weight\n        self.rc_weight = rc_weight\n        self.ce_weight = ce_weight\n\n        self.train_loss = MeanMetric()\n        self.val_loss = MeanMetric()\n        self.test_loss = MeanMetric()\n        \n        self.val_psnr = PeakSignalNoiseRatio()\n        self.val_ssim = StructuralSimilarityIndexMeasure()\n        self.train_psnr = PeakSignalNoiseRatio()\n        self.train_ssim = StructuralSimilarityIndexMeasure()\n\n        self.criterion = nn.CrossEntropyLoss() # for classifier\n        # self.classifier = FashionClassifier()\n        # self.load_classifier(\"/kaggle/input/fashionclassifier/pytorch/default/3/classifier.ckpt\")\n\n    def forward(self, batch):\n        x, y = batch\n        y_one_hot = F.one_hot(y, num_classes=10)\n        recons, input, mu, log_var = self.net(x, y_one_hot)\n        loss, rc_loss, kl_loss = self.net.loss_function(recons, input, mu, log_var)\n        return recons, loss, rc_loss, kl_loss\n\n    def load_classifier(self, ckpt_path):\n        checkpoint = torch.load(ckpt_path, map_location=self.device)\n        checkpoint_state_dict = checkpoint['state_dict']\n        checkpoint_state_dict = {k.replace('net.', ''): v for k, v in checkpoint_state_dict.items()}\n        \n        # Load the state dict into the model\n        self.classifier.load_state_dict(checkpoint_state_dict)\n        self.classifier.to(self.device)\n        self.classifier.eval()\n        \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        recons, loss, recons_loss, kl_loss = self.forward(batch)\n        \n        # rc_loss = (x * torch.log(recons + epsilon) + (1 - x) * torch.log(1 - recons + epsilon)).mean()\n        \n        # preds = classifier(recons)\n        # ce_loss = F.cross_entropy(preds, y)\n        # loss = self.rc_weight*rc_loss + self.kl_weight*kl_loss\n        # total_loss = self.rc_weight*rc_loss + self.kl_weight*kl_loss + self.ce_weight*ce_loss\n        self.train_loss(loss)\n        self.log(\"train/loss\", self.train_loss, on_step=False, on_epoch=True, prog_bar=True)\n\n        if batch_idx%10 == 0:\n            psnr_value = self.train_psnr(recons, x)\n            ssim_value = self.train_ssim(recons, x)\n            \n            reconstruction = make_grid(recons, nrow=10, normalize=True)\n            x = make_grid(x, nrow=10, normalize=True)\n            self.logger.log_image(key='train/image', images=[reconstruction, x], caption=['reconstruction','real'])        \n            self.log(\"train/loss\", self.train_loss, on_step=False, on_epoch=True, prog_bar=True)\n            self.log(\"train/psnr\", psnr_value, on_step=False, on_epoch=True, prog_bar=True)\n            self.log(\"train/ssim\", ssim_value, on_step=False, on_epoch=True, prog_bar=True)\n\n        return loss\n    \n\n    def on_train_epoch_end(self):\n        \"Lightning hook that is called when a validation epoch ends.\"\n        self.train_psnr.reset()\n        self.train_ssim.reset()\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        recons, loss, recons_loss, kl_loss = self.forward(batch)\n        \n        # rc_loss = (x * torch.log(recons) + (1 - x) * torch.log(1 - recons)).mean()\n        \n        # preds = classifier(recons)\n        # ce_loss = F.cross_entropy(preds, y)\n\n        # loss = self.rc_weight*rc_loss + self.kl_weight*kl_loss\n        # total_loss = self.rc_weight*rc_loss + self.kl_weight*kl_loss + self.ce_weight*ce_loss\n        self.val_loss(loss)\n        self.log(\"val/loss\", self.val_loss, on_step=False, on_epoch=True, prog_bar=True)\n\n        if batch_idx%10 == 0:\n            psnr_value = self.val_psnr(recons, x)\n            ssim_value = self.val_ssim(recons, x)\n            \n            reconstruction = make_grid(recons, nrow=10, normalize=True)\n            x = make_grid(x, nrow=10, normalize=True)\n            self.logger.log_image(key='val/image', images=[reconstruction, x], caption=['reconstruction','real'])        \n            self.log(\"val/loss\", self.val_loss, on_step=False, on_epoch=True, prog_bar=True)\n            self.log(\"val/psnr\", psnr_value, on_step=False, on_epoch=True, prog_bar=True)\n            self.log(\"val/ssim\", ssim_value, on_step=False, on_epoch=True, prog_bar=True)\n\n    def on_validation_epoch_end(self):\n        \"Lightning hook that is called when a validation epoch ends.\"\n        self.val_psnr.reset()\n        self.val_ssim.reset()\n\n    def test_step(self,  batch, batch_idx):\n        x, y = batch\n        recons, loss, recons_loss, kl_loss = self.forward(batch)\n        \n        # rc_loss = (x * torch.log(recons) + (1 - x) * torch.log(1 - recons)).mean()\n        \n        # preds = classifier(recons)\n        # ce_loss = F.cross_entropy(preds, y)\n        # loss = self.rc_weight*rc_loss + self.kl_weight*kl_loss\n        # total_loss = self.rc_weight*rc_loss + self.kl_weight*kl_loss + self.ce_weight*ce_loss\n        self.test_loss(loss)\n        self.log(\"test/loss\", self.test_loss, on_step=False, on_epoch=True, prog_bar=True)\n\n    def configure_optimizers(self):\n        optimizer=torch.optim.Adam(params=self.parameters(), lr=1e-4, weight_decay=1e-5)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.1, patience=10)\n      \n        return {\n           \"optimizer\": optimizer,\n           \"lr_scheduler\": {\n               \"scheduler\": scheduler,\n               \"monitor\": \"val/loss\",\n           },\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T17:56:18.044131Z","iopub.execute_input":"2024-12-17T17:56:18.044706Z","iopub.status.idle":"2024-12-17T17:56:18.060873Z","shell.execute_reply.started":"2024-12-17T17:56:18.044672Z","shell.execute_reply":"2024-12-17T17:56:18.060126Z"}},"outputs":[],"execution_count":479},{"id":"7ffd249e-7bf6-4545-9d1b-215d3e9dd845","cell_type":"code","source":"checkpoint_callback = ModelCheckpoint(\n   dirpath=\"checkpoints\",\n   monitor=\"train/psnr\",\n   filename=\"cVAE-{epoch:02d}-{val_loss:.2f}-{val_acc:.2f}\",\n   save_top_k=3,\n   mode=\"max\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T17:56:18.883774Z","iopub.execute_input":"2024-12-17T17:56:18.884748Z","iopub.status.idle":"2024-12-17T17:56:18.889577Z","shell.execute_reply.started":"2024-12-17T17:56:18.884710Z","shell.execute_reply":"2024-12-17T17:56:18.888885Z"}},"outputs":[],"execution_count":480},{"id":"ba6cda3c-d79a-4e0b-8b67-2ccb86d3936d","cell_type":"code","source":"# Get the current date and time\ncurrent_datetime = datetime.now(timezone(\"UTC\")).strftime(\"%Y-%m-%d_%H-%M-%S\")\n\n# Initialize the WandbLogger\nlogger = WandbLogger(project=\"puvae\", name=f\"run-{current_datetime}\", tags= [\"fashion\", \"cvae\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T17:56:19.468028Z","iopub.execute_input":"2024-12-17T17:56:19.468801Z","iopub.status.idle":"2024-12-17T17:56:19.473544Z","shell.execute_reply.started":"2024-12-17T17:56:19.468767Z","shell.execute_reply":"2024-12-17T17:56:19.472780Z"}},"outputs":[],"execution_count":481},{"id":"a0667978-b921-4646-83db-eb69602685ee","cell_type":"code","source":"early_stopping = EarlyStopping(monitor=\"train/psnr\", patience=20, mode=\"max\", verbose=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T17:56:20.019032Z","iopub.execute_input":"2024-12-17T17:56:20.019804Z","iopub.status.idle":"2024-12-17T17:56:20.024619Z","shell.execute_reply.started":"2024-12-17T17:56:20.019768Z","shell.execute_reply":"2024-12-17T17:56:20.023822Z"}},"outputs":[],"execution_count":482},{"id":"2bccde1f-50fc-4534-87c5-de79024a3b90","cell_type":"code","source":"model = cVAEModule().to(DEVICE)\ntrainer = pl.Trainer(\n    max_epochs=200,\n    logger=logger,\n    callbacks = [checkpoint_callback, early_stopping],\n    accumulate_grad_batches=2,\n    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n    devices=\"auto\",\n)\n\ntrainer.fit(model, train_dataloaders=train_loader, val_dataloaders=test_loader)\n\nlogger.experiment.save(\"checkpoints/*.ckpt\")\nwandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T17:56:20.717084Z","iopub.execute_input":"2024-12-17T17:56:20.717439Z","iopub.status.idle":"2024-12-17T17:56:21.337640Z","shell.execute_reply.started":"2024-12-17T17:56:20.717405Z","shell.execute_reply":"2024-12-17T17:56:21.336154Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b589c8c552c4705b160f66f97ba0801"}},"metadata":{}},{"name":"stdout","text":"Decoder inputtorch.Size([64, 2048])\nDecoder viewtorch.Size([64, 512, 2, 2])\nDecoder torch.Size([64, 32, 32, 32])\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[483], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m cVAEModule()\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m      3\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,\n\u001b[1;32m      4\u001b[0m     logger\u001b[38;5;241m=\u001b[39mlogger,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     devices\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m logger\u001b[38;5;241m.\u001b[39mexperiment\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoints/*.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     50\u001b[0m     _call_teardown_hook(trainer)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    570\u001b[0m     ckpt_path,\n\u001b[1;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m )\n\u001b[0;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    986\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1023\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1023\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1025\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1052\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1049\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1052\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1054\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:178\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:135\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:396\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    390\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    395\u001b[0m )\n\u001b[0;32m--> 396\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:319\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 319\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    322\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:411\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[479], line 76\u001b[0m, in \u001b[0;36mcVAEModule.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidation_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[1;32m     75\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m---> 76\u001b[0m     recons, loss, recons_loss, kl_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m# rc_loss = (x * torch.log(recons) + (1 - x) * torch.log(1 - recons)).mean()\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     \n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# preds = classifier(recons)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# loss = self.rc_weight*rc_loss + self.kl_weight*kl_loss\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# total_loss = self.rc_weight*rc_loss + self.kl_weight*kl_loss + self.ce_weight*ce_loss\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_loss(loss)\n","Cell \u001b[0;32mIn[479], line 28\u001b[0m, in \u001b[0;36mcVAEModule.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     26\u001b[0m x, y \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     27\u001b[0m y_one_hot \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mone_hot(y, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m recons, \u001b[38;5;28minput\u001b[39m, mu, log_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_one_hot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m loss, rc_loss, kl_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39mloss_function(recons, \u001b[38;5;28minput\u001b[39m, mu, log_var)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m recons, loss, rc_loss, kl_loss\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[478], line 129\u001b[0m, in \u001b[0;36mConditionalVAE.forward\u001b[0;34m(self, input, y)\u001b[0m\n\u001b[1;32m    125\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreparameterize(mu, log_var)\n\u001b[1;32m    127\u001b[0m z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([z, y], dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28minput\u001b[39m, mu, log_var\n","Cell \u001b[0;32mIn[478], line 101\u001b[0m, in \u001b[0;36mConditionalVAE.decode\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m     99\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(result) \u001b[38;5;66;03m# [64, 128, 16, 16]\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecoder \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 101\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinal_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# [64, 1, 32, 32]\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:948\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    943\u001b[0m num_spatial_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    944\u001b[0m output_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_padding(\n\u001b[1;32m    945\u001b[0m     \u001b[38;5;28minput\u001b[39m, output_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    946\u001b[0m     num_spatial_dims, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_transpose2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_padding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: output padding must be smaller than either stride or dilation, but got output_padding_height: 1 output_padding_width: 1 stride_height: 1 stride_width: 1 dilation_height: 1 dilation_width: 1"],"ename":"RuntimeError","evalue":"output padding must be smaller than either stride or dilation, but got output_padding_height: 1 output_padding_width: 1 stride_height: 1 stride_width: 1 dilation_height: 1 dilation_width: 1","output_type":"error"}],"execution_count":483},{"id":"85cca637-0ffb-42ce-99d8-8ac9e4ab4284","cell_type":"markdown","source":"# Original VAE","metadata":{}},{"id":"48082911-bcbd-4e37-aba9-c58b490cc1e6","cell_type":"code","source":"class Sampling(nn.Module):\n    def forward(self, z_mean, z_log_var, training):\n        sigma_epsilon = 1.0 if training else 0.1\n        epsilon = torch.randn_like(z_mean) * sigma_epsilon\n        return z_mean + torch.exp(0.5 * torch.clamp(z_log_var, min=-10, max=10)) * epsilon\n    \nclass Encoder(nn.Module):\n    def __init__(self, latent_dim=32, image_size:int=28, channels:int=1, kernel_size:int=7, training:bool=True):\n        super(Encoder, self).__init__()\n        self.latent_dim = latent_dim\n        self.channels = channels\n        self.kernel_size = kernel_size\n        self.image_size = image_size\n        self.training = training\n\n        \n        self.conv1 = nn.Conv2d(self.channels, 32, kernel_size=self.kernel_size, padding='same', dilation=2)\n\n        self.conv2 = nn.Conv2d(32, 32, kernel_size=self.kernel_size, dilation=2, padding='same')\n        self.conv3 = nn.Conv2d(32, 32, kernel_size=self.kernel_size, dilation=2, padding='same')\n        self.conv4 = nn.Conv2d(32, 32, kernel_size=self.kernel_size, dilation=2, padding='same')\n        \n        self.flatten = nn.Flatten()\n        self.dense1 = nn.Linear(self.latent_dim * self.image_size * self.image_size + 10, 1024)\n        self.dense2 = nn.Linear(1024, 1024)\n\n        self.dense3 = nn.Linear(1024, self.latent_dim)\n        self.dense4 = nn.Linear(1024, self.latent_dim)\n        self.softplus = nn.Softplus()\n        self.sampling = Sampling()\n\n    def forward(self, x, y):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n\n        \n        x = F.relu(self.conv4(x))\n            \n        x = self.flatten(x)\n        y_one_hot = F.one_hot(y, num_classes=len(labels))\n        x = torch.cat([x, y_one_hot], dim=-1)\n        \n        x = F.relu(self.dense1(x))\n        x = F.relu(self.dense2(x))\n\n        z_mean = self.dense3(x)\n        z_log_var = self.softplus(self.dense4(x))\n        z = self.sampling(z_mean, z_log_var, self.training)\n        return z_mean, z_log_var, z","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T15:08:50.561804Z","iopub.execute_input":"2024-12-17T15:08:50.562440Z","iopub.status.idle":"2024-12-17T15:08:50.574231Z","shell.execute_reply.started":"2024-12-17T15:08:50.562405Z","shell.execute_reply":"2024-12-17T15:08:50.573396Z"}},"outputs":[],"execution_count":205},{"id":"72e4c23e-65da-432f-8736-78410fd84efe","cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, \n                 latent_dim: int=32, \n                 channels: int=1, \n                 image_size:int=28, \n                 kernel_size:int=7, \n                 hidden_unit:int=512):\n        super(Decoder, self).__init__()\n        self.latent_dim = latent_dim\n        self.channels = channels\n        self.image_size = image_size\n        self.kernel_size = kernel_size\n        self.hidden_unit = hidden_unit\n        \n        self.fc = nn.Linear(32 + 10, self.hidden_unit)  # Adjust based on latent_dim\n        self.deconv1 = nn.ConvTranspose2d(self.hidden_unit, 32, kernel_size=kernel_size, stride=2, padding=0)\n        self.deconv2 = nn.ConvTranspose2d(32, 32, kernel_size=4, stride=2, padding=1, dilation=1)\n        self.deconv3 = nn.ConvTranspose2d(32, 32, kernel_size=4, stride=2, padding=1)\n        self.deconv4 = nn.ConvTranspose2d(32, 1, kernel_size=3, padding=1)\n\n    def forward(self, x, y):\n        \n        y_one_hot = F.one_hot(y, num_classes=10)\n        x1 = torch.cat((x, y_one_hot), dim=1)\n        x2 = F.relu(self.fc(x1))\n\n        x3 = x2.view(-1, self.hidden_unit, 1, 1)\n        \n        x4 = F.relu(self.deconv1(x3))\n        x5 = F.relu(self.deconv2(x4))\n        x6 = F.relu(self.deconv3(x5))\n        x7 = self.deconv4(x6)      \n        return torch.sigmoid(x7)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T15:08:51.177746Z","iopub.execute_input":"2024-12-17T15:08:51.178087Z","iopub.status.idle":"2024-12-17T15:08:51.186674Z","shell.execute_reply.started":"2024-12-17T15:08:51.178056Z","shell.execute_reply":"2024-12-17T15:08:51.186070Z"}},"outputs":[],"execution_count":206},{"id":"6c74a673-088e-4e4a-a751-4ef49eb0d270","cell_type":"code","source":"class PuVAE(nn.Module):\n    def __init__(self, kl_weight:int=0.1, rc_weight:int=0.01):\n        super(PuVAE, self).__init__()\n        self.kl_weight = kl_weight\n        self.rc_weight = rc_weight\n        self.encoder = Encoder()\n        self.decoder = Decoder()\n        \n    def forward(self, x, y):\n        z_mean, z_log_var, z= self.encoder(x, y)\n        recons = self.decoder(z, y)\n        kl_loss = torch.mean(z_mean**2 + torch.exp(z_log_var) - 1 - z_log_var)\n        return recons, kl_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T15:08:51.895032Z","iopub.execute_input":"2024-12-17T15:08:51.895386Z","iopub.status.idle":"2024-12-17T15:08:51.902267Z","shell.execute_reply.started":"2024-12-17T15:08:51.895356Z","shell.execute_reply":"2024-12-17T15:08:51.901583Z"}},"outputs":[],"execution_count":207},{"id":"ce0101af-69f2-4af0-aa20-12051b8eaba3","cell_type":"code","source":"from torchvision.utils import make_grid\n\nclass PuVAEModule(pl.LightningModule):\n    def __init__(self, kl_weight:int=0.0025, rc_weight:int=0.1, ce_weight: int=10.0):\n        super().__init__()\n        self.net = PuVAE()\n        self.criterion = nn.CrossEntropyLoss() # for classifier\n        self.kl_weight = kl_weight\n        self.rc_weight = rc_weight\n        self.ce_weight = ce_weight\n\n        self.train_loss = MeanMetric()\n        self.val_loss = MeanMetric()\n        self.test_loss = MeanMetric()\n        \n        self.val_psnr = PeakSignalNoiseRatio()\n        self.val_ssim = StructuralSimilarityIndexMeasure()\n        self.train_psnr = PeakSignalNoiseRatio()\n        self.train_ssim = StructuralSimilarityIndexMeasure()\n\n        # self.classifier = FashionClassifier()\n        # self.load_classifier(\"/kaggle/input/fashionclassifier/pytorch/default/3/classifier.ckpt\")\n\n    def forward(self, batch):\n        x, y = batch\n        return self.net(x, y)\n\n    def load_classifier(self, ckpt_path):\n        checkpoint = torch.load(ckpt_path, map_location=self.device)\n        checkpoint_state_dict = checkpoint['state_dict']\n        checkpoint_state_dict = {k.replace('net.', ''): v for k, v in checkpoint_state_dict.items()}\n        \n        # Load the state dict into the model\n        self.classifier.load_state_dict(checkpoint_state_dict)\n        self.classifier.to(self.device)\n        self.classifier.eval()\n        \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        recons, kl_loss = self.forward(batch)\n        epsilon = 1e-8\n        rc_loss = F.mse_loss(recons, x)\n        \n        \n        # preds = classifier(recons)\n        # ce_loss = F.cross_entropy(preds, y)\n        loss = self.rc_weight*rc_loss + self.kl_weight*kl_loss\n        # total_loss = self.rc_weight*rc_loss + self.kl_weight*kl_loss + self.ce_weight*ce_loss\n        self.train_loss(loss)\n        self.log(\"train/loss\", self.train_loss, on_step=False, on_epoch=True, prog_bar=True)\n\n        if batch_idx%10 == 0:\n            psnr_value = self.train_psnr(recons, x)\n            ssim_value = self.train_ssim(recons, x)\n            \n            reconstruction = make_grid(recons, nrow=10, normalize=True)\n            x = make_grid(x, nrow=10, normalize=True)\n            self.logger.log_image(key='train/image', images=[reconstruction, x], caption=['reconstruction','real'])        \n            self.log(\"train/loss\", self.train_loss, on_step=False, on_epoch=True, prog_bar=True)\n            self.log(\"train/psnr\", psnr_value, on_step=False, on_epoch=True, prog_bar=True)\n            self.log(\"train/ssim\", ssim_value, on_step=False, on_epoch=True, prog_bar=True)\n\n        return loss\n    \n\n    def on_train_epoch_end(self):\n        \"Lightning hook that is called when a validation epoch ends.\"\n        self.train_psnr.reset()\n        self.train_ssim.reset()\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        recons, kl_loss = self.forward(batch)\n        rc_loss = F.mse_loss(recons, x)\n        \n        \n        # preds = classifier(recons)\n        # ce_loss = F.cross_entropy(preds, y)\n\n        loss = self.rc_weight*rc_loss + self.kl_weight*kl_loss\n        # total_loss = self.rc_weight*rc_loss + self.kl_weight*kl_loss + self.ce_weight*ce_loss\n        self.val_loss(loss)\n        self.log(\"val/loss\", self.val_loss, on_step=False, on_epoch=True, prog_bar=True)\n\n        if batch_idx%10 == 0:\n            psnr_value = self.val_psnr(recons, x)\n            ssim_value = self.val_ssim(recons, x)\n            \n            reconstruction = make_grid(recons, nrow=10, normalize=True)\n            x = make_grid(x, nrow=10, normalize=True)\n            self.logger.log_image(key='val/image', images=[reconstruction, x], caption=['reconstruction','real'])        \n            self.log(\"val/loss\", self.val_loss, on_step=False, on_epoch=True, prog_bar=True)\n            self.log(\"val/psnr\", psnr_value, on_step=False, on_epoch=True, prog_bar=True)\n            self.log(\"val/ssim\", ssim_value, on_step=False, on_epoch=True, prog_bar=True)\n\n    def on_validation_epoch_end(self):\n        \"Lightning hook that is called when a validation epoch ends.\"\n        self.val_psnr.reset()\n        self.val_ssim.reset()\n\n    def test_step(self,  batch, batch_idx):\n        x, y = batch\n        recons, kl_loss = self.forward(batch)\n        rc_loss = F.mse_loss(recons, x)\n        \n        \n        # preds = classifier(recons)\n        # ce_loss = F.cross_entropy(preds, y)\n        loss = self.rc_weight*rc_loss + self.kl_weight*kl_loss\n        # total_loss = self.rc_weight*rc_loss + self.kl_weight*kl_loss + self.ce_weight*ce_loss\n        self.test_loss(loss)\n        self.log(\"test/loss\", self.test_loss, on_step=False, on_epoch=True, prog_bar=True)\n\n    def configure_optimizers(self):\n        optimizer=torch.optim.Adam(params=self.parameters(), lr=1e-4, weight_decay=1e-5)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.1, patience=10)\n      \n        return {\n           \"optimizer\": optimizer,\n           \"lr_scheduler\": {\n               \"scheduler\": scheduler,\n               \"monitor\": \"val/loss\",\n           },\n        }\n\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T15:47:22.615236Z","iopub.execute_input":"2024-12-17T15:47:22.615641Z","iopub.status.idle":"2024-12-17T15:47:22.633541Z","shell.execute_reply.started":"2024-12-17T15:47:22.615601Z","shell.execute_reply":"2024-12-17T15:47:22.632602Z"}},"outputs":[],"execution_count":219},{"id":"c46036ac-0826-4ae3-8c35-0ec8eded9c60","cell_type":"code","source":"checkpoint_callback = ModelCheckpoint(\n   dirpath=\"checkpoints\",\n   monitor=\"train/psnr\",\n   filename=\"puVAE-{epoch:02d}-{val_loss:.2f}-{val_acc:.2f}\",\n   save_top_k=3,\n   mode=\"min\",\n)","metadata":{"id":"TpG36XjrmoGY","trusted":true,"execution":{"iopub.status.busy":"2024-12-17T15:47:23.221689Z","iopub.execute_input":"2024-12-17T15:47:23.222287Z","iopub.status.idle":"2024-12-17T15:47:23.227828Z","shell.execute_reply.started":"2024-12-17T15:47:23.222240Z","shell.execute_reply":"2024-12-17T15:47:23.226906Z"}},"outputs":[],"execution_count":220},{"id":"852a4eb1-ed62-4a8a-a708-350041589fdc","cell_type":"code","source":"# Get the current date and time\ncurrent_datetime = datetime.now(timezone(\"UTC\")).strftime(\"%Y-%m-%d_%H-%M-%S\")\n\n# Initialize the WandbLogger\nlogger = WandbLogger(project=\"puvae\", name=f\"run-{current_datetime}\", tags= [\"fashion\", \"puvae\"])","metadata":{"id":"47Dr-eXPHiaC","trusted":true,"execution":{"iopub.status.busy":"2024-12-17T15:47:23.827127Z","iopub.execute_input":"2024-12-17T15:47:23.827987Z","iopub.status.idle":"2024-12-17T15:47:23.833181Z","shell.execute_reply.started":"2024-12-17T15:47:23.827948Z","shell.execute_reply":"2024-12-17T15:47:23.832185Z"}},"outputs":[],"execution_count":221},{"id":"391e9cca-99cb-4b7a-bd80-526949cfe779","cell_type":"code","source":"early_stopping = EarlyStopping(monitor=\"train/psnr\", patience=10, mode=\"min\", verbose=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T15:47:24.405486Z","iopub.execute_input":"2024-12-17T15:47:24.406309Z","iopub.status.idle":"2024-12-17T15:47:24.411176Z","shell.execute_reply.started":"2024-12-17T15:47:24.406254Z","shell.execute_reply":"2024-12-17T15:47:24.410334Z"}},"outputs":[],"execution_count":222},{"id":"4ef2e13a-1e59-4059-b62c-35d6e189e657","cell_type":"code","source":"classifier = classifier.to(DEVICE)\nmodel = PuVAEModule().to(DEVICE)\ntrainer = pl.Trainer(\n    max_epochs=200,\n    logger=logger,\n    callbacks = [checkpoint_callback, early_stopping],\n    accumulate_grad_batches=2,\n    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n    devices=\"auto\",\n)\n\ntrainer.fit(model, train_dataloaders=train_loader, val_dataloaders=test_loader)\n\nlogger.experiment.save(\"checkpoints/*.ckpt\")\nwandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T15:47:25.000970Z","iopub.execute_input":"2024-12-17T15:47:25.001794Z","iopub.status.idle":"2024-12-17T16:22:50.053745Z","shell.execute_reply.started":"2024-12-17T15:47:25.001760Z","shell.execute_reply":"2024-12-17T16:22:50.052173Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3def60a0d0674c79b53f2bb40de4606f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    570\u001b[0m     ckpt_path,\n\u001b[1;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m )\n\u001b[0;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1025\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1025\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:250\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m     batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:190\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:268\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:167\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 167\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/core/module.py:1306\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;124;03mthe optimizer.\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1304\u001b[0m \n\u001b[1;32m   1305\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1306\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py:153\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:238\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py:122\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m             )\n\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py:89\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 89\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adam.py:205\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 205\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py:108\u001b[0m, in \u001b[0;36mPrecision._wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03mhook is called.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m \n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:144\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:129\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39menable_grad()\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 129\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:317\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[0;32m--> 317\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:319\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 319\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:390\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[219], line 49\u001b[0m, in \u001b[0;36mPuVAEModule.training_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# total_loss = self.rc_weight*rc_loss + self.kl_weight*kl_loss + self.ce_weight*ce_loss\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain/loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loss, on_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, on_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, prog_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchmetrics/metric.py:316\u001b[0m, in \u001b[0;36mMetric.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_reduce_state_update\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchmetrics/metric.py:385\u001b[0m, in \u001b[0;36mMetric._forward_reduce_state_update\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;66;03m# calculate batch state and compute batch value\u001b[39;00m\n\u001b[0;32m--> 385\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m batch_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchmetrics/metric.py:550\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 550\u001b[0m     \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchmetrics/aggregation.py:563\u001b[0m, in \u001b[0;36mMeanMetric.update\u001b[0;34m(self, value, weight)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(weight, Tensor):\n\u001b[0;32m--> 563\u001b[0m     weight \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m weight \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbroadcast_to(weight, value\u001b[38;5;241m.\u001b[39mshape)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[223], line 12\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m PuVAEModule()\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m      4\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,\n\u001b[1;32m      5\u001b[0m     logger\u001b[38;5;241m=\u001b[39mlogger,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     devices\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m logger\u001b[38;5;241m.\u001b[39mexperiment\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoints/*.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[1;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     67\u001b[0m     _interrupt(trainer, exception)\n","\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"],"ename":"NameError","evalue":"name 'exit' is not defined","output_type":"error"}],"execution_count":223},{"id":"7b48149e-a110-4f61-9dd6-497ff01fe036","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}